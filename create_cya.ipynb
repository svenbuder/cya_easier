{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4857233c",
   "metadata": {},
   "source": [
    "# create $cya$ - \"Compare Your Abundances\" creation\n",
    "\n",
    "The intend of this notebook is to read in existing high-quality literature data of elemental abundances and homogenise them to use the same keywords.\n",
    "They are then stored in directories for individual elements to allow everyone to just read in the relevant ones for each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "596118a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preamble \n",
    "try:\n",
    "    %matplotlib inline\n",
    "    %config InlineBackend.figure_format='retina'\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# general packages\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import glob\n",
    "from astroquery.vizier import Vizier\n",
    "\n",
    "# astropy\n",
    "from astropy.table import Table,join,hstack,vstack\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.io import fits\n",
    "import astropy.units as u\n",
    "from astropy.units import UnitsWarning\n",
    "warnings.filterwarnings('ignore', category=UnitsWarning, module='astropy.units.core')\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from matplotlib import rcParams\n",
    "rcParams['axes.labelsize'] = 15\n",
    "rcParams['legend.fontsize'] = 15\n",
    "rcParams['figure.titlesize'] = 20\n",
    "\n",
    "panels = [\n",
    "    'a)','b)','c)',\n",
    "    'd)','e)','f)',\n",
    "    'g)','h)','i)',\n",
    "    'j)','k)','l)',\n",
    "    'm)','n)','o)',\n",
    "    'q)','r)','s)',\n",
    "    't)','u)','v)',\n",
    "    'w)','x)','y)',\n",
    "    'z)','aa)','ab)',\n",
    "    'ac)','ad)','ae)'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a18f5637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the elements reported by the APOGEE and GALAH surveys\n",
    "elements = [\n",
    "    'Li',\n",
    "    'C','N','O',\n",
    "    'Na','Mg','Al','Si','P','S',\n",
    "    'K','Ca','Sc','Ti','V','Cr','Mn','Fe','Co','Ni','Cu','Zn',\n",
    "    'Rb','Sr','Y','Zr','Mo','Ru',\n",
    "    'Ba',\n",
    "    'La','Ce','Nd','Sm','Eu','Yb'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3422f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are we overwriting existing homogenised FITS tables?\n",
    "overwrite_existing = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "724c8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are the mandatory keywords that every FITS table needs to have?\n",
    "mandatory_keywords = [\n",
    "    'tmass_id',\n",
    "    'ra',\n",
    "    'dec',\n",
    "    'teff',\n",
    "    'logg',\n",
    "    'fe_h',\n",
    "    'e_fe_h'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb69902",
   "metadata": {},
   "source": [
    "# 1. Literature Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2ed530",
   "metadata": {},
   "source": [
    "## 1.1 Download Literature from VizieR\n",
    "\n",
    "The literature data is typically downloaded from the VizieR interface (https://vizier.cds.unistra.fr) with the following preferences:  \n",
    "- max : unlimited entries  \n",
    "- type : FITS (binary) Table\n",
    "- all column check\n",
    "\n",
    "as a binary FITS table with all VizieR columns, that is, also including _RA and _DE coordinates for J2000.\n",
    "\n",
    "Follow the directory and path convention:\n",
    "\n",
    "```\n",
    "literature_raw_files/\"+LastNameOfFirstAuthor+\"_\"+ADS_Bibcode+\"_VizieR.fits\n",
    "```\n",
    "\n",
    "For example, for the Bensby et al. (2014) work with ADS_Bibcode 2014A&A...562A..71B:\n",
    "```\n",
    "literature_raw_files/Bensby_2014A&A...562A..71B_VizieR.fits\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "e48437a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Battistini_2015A&A...577A...9B',\n",
       " 'Battistini_2016A&A...586A..49B',\n",
       " 'Bensby_2014A&A...562A..71B',\n",
       " 'Bensby_2017A&A...605A..89B',\n",
       " 'Bensby_2018A%26A...615A.151B',\n",
       " 'Bensby_2020A&A...634A.130B',\n",
       " 'Bensby_2021A&A...655A.117B',\n",
       " 'Forsberg_2019A&A...631A.113F',\n",
       " 'Forsberg_2022A&A...666A.125F',\n",
       " 'Joensson_2017A&A...598A.100J',\n",
       " 'Montelius_2022A&A...665A.135M',\n",
       " 'Nissen_2010A&A...511L..10N',\n",
       " 'Nissen_2011A&A...530A..15N',\n",
       " 'Nissen_2012A%26A...543A..28N',\n",
       " 'Nissen_2014A&A...568A..25N',\n",
       " 'Nissen_2020A&A...640A..81N',\n",
       " 'Nissen_2024A&A...682A.116N']"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify all available literature\n",
    "# Note that we expect the file paths\n",
    "# to start with the 21 characters \"literature_raw_files/\"\n",
    "#  and end with the 12 characters \"_VizieR.fits\"\n",
    "available_literature = [x[21:-12] for x in glob.glob('literature_raw_files/*_VizieR.fits')]\n",
    "available_literature.sort()\n",
    "available_literature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da44594",
   "metadata": {},
   "source": [
    "## 1.2 Concatenating of extensions\n",
    "\n",
    "Some authors upload their data in multiple tables -> this will create multiple FITS extensions.  \n",
    "If they do, the code of Section 1.3 will fail and tell you to create a VizieR FITS File with only one extension.\n",
    "I would suggest to either merge extensions in TOPCAT, or with astropy.table.Table\n",
    "\n",
    "This has been done to the following VizieR.fits files:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "40d3ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_extra_fits_extensions(literature_file):\n",
    "    \n",
    "    fits_file = fits.open('literature_raw_files/'+literature_file+'_VizieR.fits')\n",
    "    \n",
    "    if len(fits_file) <= 2:\n",
    "        print('1 FITS extension for '+literature_file+'.')\n",
    "        table = Table.read('literature_raw_files/'+literature_file+'_VizieR.fits')\n",
    "        table.write('literature_raw_files/'+literature_file+'_VizieR_merged.fits', overwrite = True)\n",
    "    else:\n",
    "        if literature_file in [\n",
    "            'Bensby_2014A&A...562A..71B','Bensby_2017A&A...605A..89B',\n",
    "            'Forsberg_2019A&A...631A.113F','Forsberg_2022A&A...666A.125F',\n",
    "            'Montelius_2022A&A...665A.135M',\n",
    "            'Nissen_2010A&A...511L..10N','Nissen_2011A&A...530A..15N',\n",
    "            'Nissen_2014A&A...568A..25N','Nissen_2020A&A...640A..81N'\n",
    "        ]:\n",
    "            print(str(len(fits_file)-1)+' FITS extensions for '+literature_file+'. But already merged.')\n",
    "        else:\n",
    "            print('--> '+str(len(fits_file)-1)+' FITS extensions for '+literature_file+'. You first need to merge them before you can continue!')\n",
    "    fits_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "8490d48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 FITS extension for Battistini_2015A&A...577A...9B.\n",
      "1 FITS extension for Battistini_2016A&A...586A..49B.\n",
      "3 FITS extensions for Bensby_2014A&A...562A..71B. But already merged.\n",
      "2 FITS extensions for Bensby_2017A&A...605A..89B. But already merged.\n",
      "1 FITS extension for Bensby_2018A%26A...615A.151B.\n",
      "1 FITS extension for Bensby_2020A&A...634A.130B.\n",
      "1 FITS extension for Bensby_2021A&A...655A.117B.\n",
      "1 FITS extension for Forsberg_2019A&A...631A.113F.\n",
      "4 FITS extensions for Forsberg_2022A&A...666A.125F. But already merged.\n",
      "1 FITS extension for Joensson_2017A&A...598A.100J.\n",
      "2 FITS extensions for Montelius_2022A&A...665A.135M. But already merged.\n",
      "2 FITS extensions for Nissen_2010A&A...511L..10N. But already merged.\n",
      "2 FITS extensions for Nissen_2011A&A...530A..15N. But already merged.\n",
      "1 FITS extension for Nissen_2012A%26A...543A..28N.\n",
      "2 FITS extensions for Nissen_2014A&A...568A..25N. But already merged.\n",
      "2 FITS extensions for Nissen_2020A&A...640A..81N. But already merged.\n",
      "1 FITS extension for Nissen_2024A&A...682A.116N.\n"
     ]
    }
   ],
   "source": [
    "for literature_file in available_literature:\n",
    "    check_for_extra_fits_extensions(literature_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c56dc",
   "metadata": {},
   "source": [
    "## 1.3 Match to the parent catalogue (i.e. either 2MASS or Gaia DR3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "1ab2afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_2mass_sources(literature_file):\n",
    "    \n",
    "    try:\n",
    "        fits_file = fits.open('literature_raw_files/'+literature_file+'_VizieR_merged.fits')\n",
    "        \n",
    "        # Some authors upload their data in multiple tables -> this will create multiple FITS extensions\n",
    "        # Check if we have multiple extensions in the VizieR file.\n",
    "        if len(fits_file) > 2:\n",
    "            raise ValueError(literature_file+' has '+str(len(fits_file))+' extensions\\nFirst join/concatenate the entries to only have 1 extension.')\n",
    "    \n",
    "        fits_file.close()\n",
    "    except:\n",
    "        raise FileNotFoundError('You first need to create a _VizieR_merged.fits file for '+literature_file)\n",
    "        \n",
    "    fits_file = Table.read('literature_raw_files/'+literature_file+'_VizieR_merged.fits')\n",
    "    \n",
    "    # First identify the Right Ascension (RA) and Declination (DE)\n",
    "    # They come with different column names of course...\n",
    "    try:\n",
    "        ra, dec = (fits_file['RAJ2000'],fits_file['DEJ2000'])\n",
    "    except:\n",
    "        try:\n",
    "            ra, dec = (fits_file['_RAJ2000'],fits_file['_DEJ2000'])\n",
    "        except:\n",
    "            ra, dec = (fits_file['_RA'],fits_file['_DE'])\n",
    "    \n",
    "    # Create an in-memory table for the coordinates\n",
    "    coords = SkyCoord(ra=ra, dec=dec, unit=(u.deg, u.deg))\n",
    "    \n",
    "    # Query Vizier for the 2MASS catalog\n",
    "    vizier = Vizier(columns=['RAJ2000', 'DEJ2000', '2MASS'])\n",
    "    vizier.ROW_LIMIT = -1  # no row limit\n",
    "    result = vizier.query_region(coords, radius=2*u.arcsec, catalog='II/246/out')\n",
    "    \n",
    "    if not result:\n",
    "        raise ValueError(\"No results found in 2MASS catalog.\")\n",
    "    \n",
    "    # Extract the result table\n",
    "    tmass_result = result[0]\n",
    "    \n",
    "    # Create a SkyCoord object for the query results\n",
    "    tmass_coords = SkyCoord(ra=tmass_result['RAJ2000'], dec=tmass_result['DEJ2000'], unit=(u.deg, u.deg))\n",
    "    \n",
    "    # Match the input coordinates to the query results\n",
    "    idx, sep, _ = coords.match_to_catalog_sky(tmass_coords)\n",
    "    \n",
    "    tmass_ids = []\n",
    "    for i, j in enumerate(idx):\n",
    "        if sep[i].arcsec < 2:\n",
    "            tmass_ids.append(tmass_result['_2MASS'][j])\n",
    "        else:\n",
    "            tmass_ids.append('')\n",
    "            \n",
    "    fits_file['tmass_id'] = np.array(tmass_ids, dtype=str)\n",
    "    \n",
    "    fits_file.write('literature_raw_files/'+literature_file+'_VizieR_merged_tmass_id.fits',overwrite=True)\n",
    "    \n",
    "    print(\"Added 2MASS ID as 'tmass_id' to \"+literature_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "cf13ebc0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2MASS ID as 'tmass_id' to Battistini_2015A&A...577A...9B\n",
      "Added 2MASS ID as 'tmass_id' to Battistini_2016A&A...586A..49B\n",
      "Added 2MASS ID as 'tmass_id' to Bensby_2014A&A...562A..71B\n",
      "Added 2MASS ID as 'tmass_id' to Bensby_2017A&A...605A..89B\n",
      "Added 2MASS ID as 'tmass_id' to Bensby_2018A%26A...615A.151B\n",
      "Added 2MASS ID as 'tmass_id' to Bensby_2020A&A...634A.130B\n",
      "Added 2MASS ID as 'tmass_id' to Bensby_2021A&A...655A.117B\n",
      "Added 2MASS ID as 'tmass_id' to Forsberg_2019A&A...631A.113F\n",
      "Added 2MASS ID as 'tmass_id' to Forsberg_2022A&A...666A.125F\n",
      "Added 2MASS ID as 'tmass_id' to Joensson_2017A&A...598A.100J\n",
      "Added 2MASS ID as 'tmass_id' to Montelius_2022A&A...665A.135M\n",
      "Added 2MASS ID as 'tmass_id' to Nissen_2010A&A...511L..10N\n",
      "Added 2MASS ID as 'tmass_id' to Nissen_2011A&A...530A..15N\n",
      "Added 2MASS ID as 'tmass_id' to Nissen_2012A%26A...543A..28N\n",
      "Added 2MASS ID as 'tmass_id' to Nissen_2014A&A...568A..25N\n",
      "Added 2MASS ID as 'tmass_id' to Nissen_2020A&A...640A..81N\n",
      "Added 2MASS ID as 'tmass_id' to Nissen_2024A&A...682A.116N\n"
     ]
    }
   ],
   "source": [
    "for literature_file in available_literature:\n",
    "    find_closest_2mass_sources(literature_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ab5968",
   "metadata": {},
   "source": [
    "## 1.4 Unified Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "4f669f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_good         = 0\n",
    "flag_upper_limit  = 1\n",
    "flag_no_detection = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "id": "ccd22f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_keywords(literature_file):\n",
    "    \"\"\"\n",
    "    This function unifies the keywords of the merged VizieR catalogues.\n",
    "    \n",
    "    This needs to be extended for new literature data.\n",
    "    \"\"\"\n",
    "    \n",
    "    raw_data = Table.read('literature_raw_files/'+literature_file+'_VizieR_merged_tmass_id.fits')\n",
    "    oid = np.arange(len(raw_data['tmass_id']))\n",
    "    \n",
    "    author, ads_bibcode = literature_file.split('_')\n",
    "    raw_data['author']      = [author+'+'+ads_bibcode[:4] for entry in oid]\n",
    "    raw_data['ads_bibcode'] = [ads_bibcode for entry in oid]\n",
    "    \n",
    "    unified_table = Table()\n",
    "    unified_keywords = dict()\n",
    "    \n",
    "    if literature_file == 'Battistini_2015A&A...577A...9B':\n",
    "        \n",
    "        unified_keywords['author']      = 'author'\n",
    "        unified_keywords['ads_bibcode'] = 'ads_bibcode'\n",
    "        unified_keywords['tmass_id']    = 'tmass_id'\n",
    "        unified_keywords['ra']          = '_RA'\n",
    "        unified_keywords['dec']         = '_DE'\n",
    "        \n",
    "        # Note: Battistini_2015A&A...577A...9B used Teff/logg/[Fe/H]/vmic from Bensby_2014A&A...562A..71B\n",
    "        # We therefore propagate the latter values (more precise) and uncertainties (not reported in the former)\n",
    "        unified_keywords['teff']        = 'Teff'\n",
    "        unified_keywords['e_teff']      = 'e_Teff'\n",
    "        unified_keywords['logg']        = 'logg'\n",
    "        unified_keywords['e_logg']      = 'logg'\n",
    "        unified_keywords['fe_h']        = 'Fe_H'\n",
    "        unified_keywords['e_fe_h']      = 'e_Fe_H'\n",
    "        unified_keywords['vmic']        = 'xi'\n",
    "        unified_keywords['e_vmic']      = 'e_xi'\n",
    "        \n",
    "        for element in ['Sc','V','Mn','Co']:\n",
    "            \n",
    "            unified_keywords[element.lower()+'_h'] = '__'+element+'_H_'\n",
    "            \n",
    "            # Errors are not reported on a star-by-star basis.\n",
    "            # We need to define the uncertainties globally.\n",
    "            # See Table 2 of Battistini et al. (2015)\n",
    "            if element == 'Sc': e_x_fe = 0.08\n",
    "            if element == 'V' : e_x_fe = 0.10\n",
    "            if element == 'Mn': e_x_fe = 0.09\n",
    "            if element == 'Co': e_x_fe = 0.07\n",
    "            raw_data['e_'+element.lower()+'_h'] = np.array([e_x_fe for entry in oid],dtype=np.float32)\n",
    "            unified_keywords['e_'+element.lower()+'_h'] = 'e_'+element.lower()+'_h'\n",
    "\n",
    "            # Set a flag for each abundance and raise by 'flag_no_detection' if not finite\n",
    "            # also ensure that uncertainties are not finite for these stars\n",
    "            unified_keywords['flag_'+element.lower()+'_h'] = 'flag_'+element.lower()+'_h'\n",
    "            raw_data['flag_'+element.lower()+'_h'] = np.zeros(len(oid),dtype=int)\n",
    "            no_abundance_detected = np.isnan(raw_data[unified_keywords[element.lower()+'_h']])\n",
    "            raw_data['flag_'+element.lower()+'_h'][no_abundance_detected] += flag_no_detection\n",
    "            raw_data['e_'+element.lower()+'_h'][no_abundance_detected] = np.nan\n",
    "            \n",
    "        unified_keywords['HIP']         = 'HIP'\n",
    "        \n",
    "    elif literature_file == 'Battistini_2016A&A...586A..49B':\n",
    "        \n",
    "        unified_keywords['author']      = 'author'\n",
    "        unified_keywords['ads_bibcode'] = 'ads_bibcode'\n",
    "        unified_keywords['tmass_id']    = 'tmass_id'\n",
    "        unified_keywords['ra']          = '_RA'\n",
    "        unified_keywords['dec']         = '_DE'\n",
    "\n",
    "        # Note: Battistini_2016A&A...586A..49B used Teff/logg/[Fe/H]/vmic from Bensby_2014A&A...562A..71B\n",
    "        # We therefore propagate the latter values (more precise) and uncertainties (not reported in the former)\n",
    "        unified_keywords['teff']        = 'Teff'\n",
    "        unified_keywords['e_teff']      = 'e_Teff'\n",
    "        unified_keywords['logg']        = 'logg'\n",
    "        unified_keywords['e_logg']      = 'logg'\n",
    "        unified_keywords['fe_h']        = 'Fe_H'\n",
    "        unified_keywords['e_fe_h']      = 'e_Fe_H'\n",
    "        unified_keywords['vmic']        = 'xi'\n",
    "        unified_keywords['e_vmic']      = 'e_xi'\n",
    "        \n",
    "        for element in ['Sr','Zr','La','Ce','Nd','Sm','Eu']:\n",
    "            \n",
    "            unified_keywords[element.lower()+'_h'] = '__'+element+'_H_'\n",
    "\n",
    "            # Errors are not reported on a star-by-star basis.\n",
    "            # We need to define the uncertainties globally.\n",
    "            # See Table 6 of Battistini et al. (2016)\n",
    "            if element == 'Sr': e_x_fe = 0.15\n",
    "            if element == 'Zr': e_x_fe = 0.12\n",
    "            if element == 'La': e_x_fe = 0.11\n",
    "            if element == 'Ce': e_x_fe = 0.12\n",
    "            if element == 'Nd': e_x_fe = 0.10\n",
    "            if element == 'Sm': e_x_fe = 0.11\n",
    "            if element == 'Eu': e_x_fe = 0.08\n",
    "                \n",
    "            raw_data['e_'+element.lower()+'_h'] = np.array([e_x_fe for entry in oid],dtype=np.float32)\n",
    "            unified_keywords['e_'+element.lower()+'_h'] = 'e_'+element.lower()+'_h'\n",
    "\n",
    "            # Set a flag for each abundance and raise by 'flag_no_detection' if not finite\n",
    "            # also ensure that uncertainties are not finite for these stars\n",
    "            unified_keywords['flag_'+element.lower()+'_h'] = 'flag_'+element.lower()+'_h'\n",
    "            raw_data['flag_'+element.lower()+'_h'] = np.zeros(len(oid),dtype=int)\n",
    "            no_abundance_detected = np.isnan(raw_data[unified_keywords[element.lower()+'_h']])\n",
    "            raw_data['flag_'+element.lower()+'_h'][no_abundance_detected] += flag_no_detection\n",
    "            raw_data['e_'+element.lower()+'_h'][no_abundance_detected] = np.nan\n",
    "\n",
    "        unified_keywords['HIP']         = 'HIP'\n",
    "        \n",
    "    elif literature_file == 'Bensby_2014A&A...562A..71B':\n",
    "        \n",
    "        unified_keywords['author']      = 'author'\n",
    "        unified_keywords['ads_bibcode'] = 'ads_bibcode'\n",
    "        unified_keywords['tmass_id']    = 'tmass_id'\n",
    "        unified_keywords['ra']          = '_RA'\n",
    "        unified_keywords['dec']         = '_DE'\n",
    "        unified_keywords['teff']        = 'Teff'\n",
    "        unified_keywords['e_teff']      = 'e_Teff'\n",
    "        unified_keywords['logg']        = 'logg'\n",
    "        unified_keywords['e_logg']      = 'logg'\n",
    "        unified_keywords['fe_h']        = 'Fe_H'\n",
    "        unified_keywords['e_fe_h']      = 'e_Fe_H'\n",
    "        unified_keywords['vmic']        = 'xi'\n",
    "        unified_keywords['e_vmic']      = 'e_xi'\n",
    "        \n",
    "        for element in ['O','Na','Mg','Al','Si','Ca','Ti','Cr','Ni','Zn','Y','Ba']:\n",
    "            \n",
    "            unified_keywords[element.lower()+'_h'] = element.lower()+'_h'\n",
    "            raw_data[element.lower()+'_h'] = raw_data[element+'_Fe'] + raw_data['Fe_H']\n",
    "\n",
    "            unified_keywords['e_'+element.lower()+'_h'] = 'e_'+element.lower()+'_h'\n",
    "            raw_data['e_'+element.lower()+'_h'] = raw_data['e_'+element+'_Fe']\n",
    "\n",
    "            # Set a flag for each abundance and raise by 'flag_no_detection' if not finite\n",
    "            # also ensure that uncertainties are not finite for these stars\n",
    "            unified_keywords['flag_'+element.lower()+'_h'] = 'flag_'+element.lower()+'_h'\n",
    "            raw_data['flag_'+element.lower()+'_h'] = np.zeros(len(oid),dtype=int)\n",
    "            no_abundance_detected = np.isnan(raw_data[unified_keywords[element.lower()+'_h']])\n",
    "            raw_data['flag_'+element.lower()+'_h'][no_abundance_detected] += flag_no_detection\n",
    "            raw_data['e_'+element.lower()+'_h'][no_abundance_detected] = np.nan\n",
    "\n",
    "        unified_keywords['HIP']         = 'HIP'\n",
    "        \n",
    "    elif literature_file == 'Bensby_2017A&A...605A..89B':\n",
    "        \n",
    "        unified_keywords['author']      = 'author'\n",
    "        unified_keywords['ads_bibcode'] = 'ads_bibcode'\n",
    "        unified_keywords['tmass_id']    = 'tmass_id'\n",
    "        unified_keywords['ra']          = 'RAJ2000'\n",
    "        unified_keywords['dec']         = 'DEJ2000'\n",
    "        unified_keywords['teff']        = 'Teff'\n",
    "        unified_keywords['e_teff']      = 'e_Teff'\n",
    "        unified_keywords['logg']        = 'logg'\n",
    "        unified_keywords['e_logg']      = 'logg'\n",
    "        unified_keywords['fe_h']        = '__Fe_H_'\n",
    "        unified_keywords['e_fe_h']      = 'e__Fe_H_'\n",
    "        unified_keywords['vmic']        = 'xit'\n",
    "        unified_keywords['e_vmic']      = 'e_xit'\n",
    "        \n",
    "        raw_data.rename_column('s__Mg_Fe_','e__Mg_Fe_')\n",
    "        \n",
    "        for element in ['Na','Mg','Al','Si','Ca','Ti','Cr','Ni','Zn','Y','Ba']:\n",
    "            \n",
    "            unified_keywords[element.lower()+'_h'] = element.lower()+'_h'\n",
    "            raw_data[element.lower()+'_h'] = raw_data['__'+element+'_Fe_'] + raw_data['__Fe_H_']\n",
    "\n",
    "            unified_keywords['e_'+element.lower()+'_h'] = 'e_'+element.lower()+'_h'\n",
    "            raw_data['e_'+element.lower()+'_h'] = raw_data['e__'+element+'_Fe_']\n",
    "\n",
    "            # Set a flag for each abundance and raise by 'flag_no_detection' if not finite\n",
    "            # also ensure that uncertainties are not finite for these stars\n",
    "            unified_keywords['flag_'+element.lower()+'_h'] = 'flag_'+element.lower()+'_h'\n",
    "            raw_data['flag_'+element.lower()+'_h'] = np.zeros(len(oid),dtype=int)\n",
    "            no_abundance_detected = np.isnan(raw_data[unified_keywords[element.lower()+'_h']])\n",
    "            raw_data['flag_'+element.lower()+'_h'][no_abundance_detected] += flag_no_detection\n",
    "            raw_data['e_'+element.lower()+'_h'][no_abundance_detected] = np.nan\n",
    "\n",
    "        unified_keywords['Name']        = 'Name'\n",
    "\n",
    "    elif literature_file == 'Bensby_2018A%26A...615A.151B':\n",
    "        \n",
    "        unified_keywords['author']      = 'author'\n",
    "        unified_keywords['ads_bibcode'] = 'ads_bibcode'\n",
    "        unified_keywords['tmass_id']    = 'tmass_id'\n",
    "        unified_keywords['ra']          = '_RA'\n",
    "        unified_keywords['dec']         = '_DE'\n",
    "\n",
    "        # Note: Bensby_2018A%26A...615A.151B used Teff/logg/[Fe/H]/vmic from Bensby_2014A&A...562A..71B\n",
    "        # We therefore propagate the latter values (more precise) and uncertainties (not reported in the former)\n",
    "        unified_keywords['teff']        = 'Teff'\n",
    "        unified_keywords['e_teff']      = 'e_Teff'\n",
    "        unified_keywords['logg']        = 'logg'\n",
    "        unified_keywords['e_logg']      = 'e_logg'\n",
    "        unified_keywords['fe_h']        = 'Fe_H'\n",
    "        unified_keywords['e_fe_h']      = 'e_Fe_H'\n",
    "        unified_keywords['vmic']        = 'xi'\n",
    "        unified_keywords['e_vmic']      = 'e_xi'\n",
    "        \n",
    "        unified_keywords['a_li']        = 'ALi'\n",
    "\n",
    "        # Uncertainties for A(Li) are reported as lower and upper values. We are assuming they are Gaussian here.\n",
    "        raw_data['e_a_li'] = np.array(raw_data['b_ali_lc'] - raw_data['b_ALi'], dtype=np.float32)\n",
    "        unified_keywords['e_a_li']      = 'e_a_li'\n",
    "        \n",
    "        # Set a flag for each abundance and raise by 'flag_no_detection' if not finite\n",
    "        # also ensure that uncertainties are not finite for these stars\n",
    "        unified_keywords['flag_a_li'] = 'flag_a_li'\n",
    "        raw_data['flag_a_li'] = np.array(raw_data['l_ALi'], dtype=np.int32) # has 0/1 set for detection/upper limit\n",
    "        no_abundance_detected = np.isnan(raw_data[unified_keywords['a_li']])\n",
    "        raw_data['flag_a_li'][no_abundance_detected] += flag_no_detection\n",
    "        raw_data['e_a_li'][no_abundance_detected] = np.nan\n",
    "\n",
    "\n",
    "        unified_keywords['HIP']         = 'HIP'\n",
    "        \n",
    "    elif literature_file == 'Bensby_2020A&A...634A.130B':\n",
    "        \n",
    "        unified_keywords['author']      = 'author'\n",
    "        unified_keywords['ads_bibcode'] = 'ads_bibcode'\n",
    "        unified_keywords['tmass_id']    = 'tmass_id'\n",
    "        unified_keywords['ra']          = '_RA'\n",
    "        unified_keywords['dec']         = '_DE'\n",
    "\n",
    "        # Note: Bensby_2020A&A...634A.130B used Teff/logg/[Fe/H]/vmic from Bensby_2017A&A...605A..89B\n",
    "        # We therefore propagate the latter values (more precise) and uncertainties (not reported in the former)\n",
    "        unified_keywords['teff']        = 'Teff'\n",
    "        unified_keywords['e_teff']      = 'e_Teff'\n",
    "        unified_keywords['logg']        = 'logg'\n",
    "        unified_keywords['e_logg']      = 'e_logg'\n",
    "        unified_keywords['fe_h']        = '__Fe_H_'\n",
    "        unified_keywords['e_fe_h']      = 'e__Fe_H_'\n",
    "        unified_keywords['vmic']        = 'xit'\n",
    "        unified_keywords['e_vmic']      = 'e_xit'\n",
    "        \n",
    "        unified_keywords['a_li']        = 'ALi'\n",
    "        \n",
    "        # Uncertainties for A(Li) are reported as lower and upper values. We are assuming they are Gaussian here.\n",
    "        raw_data['e_a_li'] = np.array(raw_data['ALiu'] - raw_data['ALil'], dtype=np.float32)\n",
    "        unified_keywords['e_a_li']      = 'e_a_li'\n",
    "        \n",
    "        # Set a flag for each abundance and raise by 'flag_no_detection' if not finite\n",
    "        # also ensure that uncertainties are not finite for these stars\n",
    "        unified_keywords['flag_a_li'] = 'flag_a_li'\n",
    "        raw_data['flag_a_li'] = np.array(raw_data['l_ALi'], dtype=np.int32) # has 0/1 set for detection/upper limit\n",
    "        no_abundance_detected = np.isnan(raw_data[unified_keywords['a_li']])\n",
    "        raw_data['flag_a_li'][no_abundance_detected] += flag_no_detection\n",
    "        raw_data['e_a_li'][no_abundance_detected] = np.nan\n",
    "\n",
    "        unified_keywords['Name']        = 'Name'\n",
    "        \n",
    "    elif literature_file == 'Bensby_2021A&A...655A.117B':\n",
    "        \n",
    "        unified_keywords['author']      = 'author'\n",
    "        unified_keywords['ads_bibcode'] = 'ads_bibcode'\n",
    "        unified_keywords['tmass_id']    = 'tmass_id'\n",
    "        unified_keywords['ra']          = '_RA'\n",
    "        unified_keywords['dec']         = '_DE'\n",
    "\n",
    "        # Note: Bensby_2021A&A...655A.117B used Teff/logg/[Fe/H]/vmic from Bensby_2017A&A...605A..89B\n",
    "        # We therefore propagate the latter values (more precise) and uncertainties (not reported in the former)\n",
    "        unified_keywords['teff']        = 'Teff'\n",
    "        unified_keywords['e_teff']      = 'e_Teff'\n",
    "        unified_keywords['logg']        = 'logg'\n",
    "        unified_keywords['e_logg']      = 'e_logg'\n",
    "        unified_keywords['fe_h']        = '__Fe_H_'\n",
    "        unified_keywords['e_fe_h']      = 'e__Fe_H_'\n",
    "        unified_keywords['vmic']        = 'xit'\n",
    "        unified_keywords['e_vmic']      = 'e_xit'\n",
    "        \n",
    "        for element in ['C','O']:\n",
    "            \n",
    "            unified_keywords[element.lower()+'_h'] = '__'+element+'_H_'\n",
    "\n",
    "            unified_keywords['e_'+element.lower()+'_h'] = 'e__'+element+'_H_'\n",
    "\n",
    "            # Set a flag for each abundance and raise by 'flag_no_detection' if not finite\n",
    "            # also ensure that uncertainties are not finite for these stars\n",
    "            unified_keywords['flag_'+element.lower()+'_h'] = 'flag_'+element.lower()+'_h'\n",
    "            raw_data['flag_'+element.lower()+'_h'] = np.zeros(len(oid),dtype=int)\n",
    "            no_abundance_detected = np.isnan(raw_data[unified_keywords[element.lower()+'_h']])\n",
    "            raw_data['flag_'+element.lower()+'_h'][no_abundance_detected] += flag_no_detection\n",
    "            raw_data[unified_keywords['e_'+element.lower()+'_h']][no_abundance_detected] = np.nan\n",
    "\n",
    "        unified_keywords['Name']        = 'Name'\n",
    "        \n",
    "    elif literature_file == 'Forsberg_2019A&A...631A.113F':\n",
    "        \n",
    "        unified_keywords['author']      = 'author'\n",
    "        unified_keywords['ads_bibcode'] = 'ads_bibcode'\n",
    "        unified_keywords['tmass_id']    = 'tmass_id'\n",
    "        unified_keywords['ra']          = 'RAJ2000'\n",
    "        unified_keywords['dec']         = 'DEJ2000'\n",
    "\n",
    "        unified_keywords['teff']        = 'Teff'\n",
    "        unified_keywords['e_teff']      = 'e_teff'\n",
    "        unified_keywords['logg']        = 'logg'\n",
    "        unified_keywords['e_logg']      = 'e_logg'\n",
    "        unified_keywords['fe_h']        = '__Fe_H_'\n",
    "        unified_keywords['e_fe_h']      = 'e_fe_h'\n",
    "        unified_keywords['vmic']        = 'Vmic'\n",
    "        unified_keywords['e_vmic']      = 'e_vmic'\n",
    "        \n",
    "        # Errors are not reported on a star-by-star basis.\n",
    "        # We need to define the uncertainties globally.\n",
    "        # See Section 3.2 of Forsberg et al. (2019)\n",
    "        raw_data['e_teff'] = np.ones(len(oid))\n",
    "        raw_data['e_logg'] = np.ones(len(oid))\n",
    "        raw_data['e_fe_h'] = np.ones(len(oid))\n",
    "        raw_data['e_vmic'] = np.ones(len(oid))\n",
    "        # for disk sample\n",
    "        disk_sample = raw_data['sample'] == 'disk'\n",
    "        raw_data['e_teff'][disk_sample] = 50\n",
    "        raw_data['e_logg'][disk_sample] = 0.15\n",
    "        raw_data['e_fe_h'][disk_sample] = 0.05\n",
    "        raw_data['e_vmic'][disk_sample] = 0.10\n",
    "        # for bulge sample\n",
    "        bulge_sample = raw_data['sample'] == 'bulge'\n",
    "        raw_data['e_teff'][bulge_sample] = 100\n",
    "        raw_data['e_logg'][bulge_sample] = 0.30\n",
    "        raw_data['e_fe_h'][bulge_sample] = 0.10\n",
    "        raw_data['e_vmic'][bulge_sample] = 0.20\n",
    "\n",
    "        for element in ['Zr','La','Ce','Eu']:\n",
    "\n",
    "            # Solar abundanes from Grevesse et al. (2015): http://adsabs.harvard.edu/abs/2015A%26A...573A..27G\n",
    "            # as discussed in Forsberg et al. (2019)\n",
    "            if element == 'Zr': A_X = 2.59\n",
    "            if element == 'La': A_X = 1.11\n",
    "            if element == 'Ce': A_X = 1.58\n",
    "            if element == 'Eu': A_X = 0.52\n",
    "            raw_data[element.lower()+'_h'] = raw_data['A_'+element+'_'] - A_X\n",
    "            unified_keywords[element.lower()+'_h'] = element.lower()+'_h'\n",
    "\n",
    "            # Errors are not reported on a star-by-star basis.\n",
    "            # We need to define the uncertainties globally.\n",
    "            # See Table 3 of Forsberg et al. (2019)\n",
    "            unified_keywords['e_'+element.lower()+'_h'] = 'e_'+element.lower()+'_h'\n",
    "            raw_data['e_'+element.lower()+'_h'] = np.ones(len(oid),dtype=float)\n",
    "\n",
    "            # for disk sample\n",
    "            disk_sample = raw_data['sample'] == 'disk'\n",
    "            if element == 'Zr': e_x_fe = 0.09\n",
    "            if element == 'La': e_x_fe = 0.06\n",
    "            if element == 'Ce': e_x_fe = 0.07\n",
    "            if element == 'Eu': e_x_fe = 0.06\n",
    "            raw_data['e_'+element.lower()+'_h'][disk_sample] = e_x_fe\n",
    "\n",
    "            # for bulge sample\n",
    "            bulge_sample = raw_data['sample'] == 'bulge'\n",
    "            if element == 'Zr': e_x_fe = 0.23\n",
    "            if element == 'La': e_x_fe = 0.15\n",
    "            if element == 'Ce': e_x_fe = 0.16\n",
    "            if element == 'Eu': e_x_fe = 0.15\n",
    "            raw_data['e_'+element.lower()+'_h'][bulge_sample] = e_x_fe\n",
    "\n",
    "            # Set a flag for each abundance and raise by 'flag_no_detection' if not finite\n",
    "            # also ensure that uncertainties are not finite for these stars\n",
    "            unified_keywords['flag_'+element.lower()+'_h'] = 'flag_'+element.lower()+'_h'\n",
    "            raw_data['flag_'+element.lower()+'_h'] = np.zeros(len(oid),dtype=int)\n",
    "            no_abundance_detected = np.isnan(raw_data[unified_keywords[element.lower()+'_h']])\n",
    "            raw_data['flag_'+element.lower()+'_h'][no_abundance_detected] += flag_no_detection\n",
    "            raw_data[unified_keywords['e_'+element.lower()+'_h']][no_abundance_detected] = np.nan\n",
    "\n",
    "        unified_keywords['Name']        = 'Name'\n",
    "        unified_keywords['Class']       = 'sample'\n",
    "        \n",
    "    elif literature_file == 'Forsberg_2022A&A...666A.125F':\n",
    "        \n",
    "        unified_keywords['author']      = 'author'\n",
    "        unified_keywords['ads_bibcode'] = 'ads_bibcode'\n",
    "        unified_keywords['tmass_id']    = 'tmass_id'\n",
    "        unified_keywords['ra']          = 'RAJ2000'\n",
    "        unified_keywords['dec']         = 'DEJ2000'\n",
    "\n",
    "        unified_keywords['teff']        = 'Teff'\n",
    "        unified_keywords['e_teff']      = 'e_teff'\n",
    "        unified_keywords['logg']        = 'logg'\n",
    "        unified_keywords['e_logg']      = 'e_logg'\n",
    "        unified_keywords['fe_h']        = '__Fe_H_'\n",
    "        unified_keywords['e_fe_h']      = 'e_fe_h'\n",
    "        unified_keywords['vmic']        = 'vmic'\n",
    "        unified_keywords['e_vmic']      = 'e_vmic'\n",
    "        \n",
    "        # Errors are not reported on a star-by-star basis.\n",
    "        # We need to define the uncertainties globally.\n",
    "        # See Section 3.2 of Forsberg et al. (2019)\n",
    "        raw_data['e_teff'] = np.ones(len(oid))\n",
    "        raw_data['e_logg'] = np.ones(len(oid))\n",
    "        raw_data['e_fe_h'] = np.ones(len(oid))\n",
    "        raw_data['e_vmic'] = np.ones(len(oid))\n",
    "        # for disk sample\n",
    "        disk_sample = raw_data['sample'] == 'disk'\n",
    "        raw_data['e_teff'][disk_sample] = 50\n",
    "        raw_data['e_logg'][disk_sample] = 0.15\n",
    "        raw_data['e_fe_h'][disk_sample] = 0.05\n",
    "        raw_data['e_vmic'][disk_sample] = 0.10\n",
    "        # for bulge sample\n",
    "        bulge_sample = raw_data['sample'] == 'bulge'\n",
    "        raw_data['e_teff'][bulge_sample] = 100\n",
    "        raw_data['e_logg'][bulge_sample] = 0.30\n",
    "        raw_data['e_fe_h'][bulge_sample] = 0.10\n",
    "        raw_data['e_vmic'][bulge_sample] = 0.20\n",
    "\n",
    "        for element in ['Mo']:\n",
    "\n",
    "            # Solar abundanes from Grevesse et al. (2015): http://adsabs.harvard.edu/abs/2015A%26A...573A..27G\n",
    "            # as discussed in Forsberg et al. (2022)\n",
    "            if element == 'Mo': A_X = 1.88\n",
    "            raw_data[element.lower()+'_h'] = raw_data['A_'+element+'_'] - A_X\n",
    "            unified_keywords[element.lower()+'_h'] = element.lower()+'_h'\n",
    "            \n",
    "            # Errors are not reported on a star-by-star basis.\n",
    "            # We need to define the uncertainties globally.\n",
    "            # See Table 2 of Forsberg et al. (2022)\n",
    "            unified_keywords['e_'+element.lower()+'_h'] = 'e_'+element.lower()+'_h'\n",
    "            raw_data['e_'+element.lower()+'_h'] = np.ones(len(oid),dtype=float)\n",
    "\n",
    "            # for disk sample\n",
    "            disk_sample = raw_data['sample'] == 'disk'\n",
    "            if element == 'Mo': e_x_fe = 0.11\n",
    "            raw_data['e_'+element.lower()+'_h'][disk_sample] = e_x_fe\n",
    "\n",
    "            # for bulge sample\n",
    "            bulge_sample = raw_data['sample'] == 'bulge'\n",
    "            if element == 'Mo': e_x_fe = 0.20\n",
    "            raw_data['e_'+element.lower()+'_h'][bulge_sample] = e_x_fe\n",
    "\n",
    "            # Set a flag for each abundance and raise by 'flag_no_detection' if not finite\n",
    "            # also ensure that uncertainties are not finite for these stars\n",
    "            unified_keywords['flag_'+element.lower()+'_h'] = 'flag_'+element.lower()+'_h'\n",
    "            raw_data['flag_'+element.lower()+'_h'] = np.zeros(len(oid),dtype=int)\n",
    "            no_abundance_detected = np.isnan(raw_data[unified_keywords[element.lower()+'_h']])\n",
    "            raw_data['flag_'+element.lower()+'_h'][no_abundance_detected] += flag_no_detection\n",
    "            raw_data[unified_keywords['e_'+element.lower()+'_h']][no_abundance_detected] = np.nan\n",
    "\n",
    "        unified_keywords['Name']        = 'Name'\n",
    "        unified_keywords['Class']       = 'sample'\n",
    "\n",
    "    elif literature_file == 'Joensson_2017A&A...598A.100J':\n",
    "        \n",
    "        unified_keywords['author']      = 'author'\n",
    "        unified_keywords['ads_bibcode'] = 'ads_bibcode'\n",
    "        unified_keywords['tmass_id']    = 'tmass_id'\n",
    "        unified_keywords['ra']          = 'RAJ2000'\n",
    "        unified_keywords['dec']         = 'DEJ2000'\n",
    "\n",
    "        unified_keywords['teff']        = 'Teff'\n",
    "        unified_keywords['e_teff']      = 'e_teff'\n",
    "        unified_keywords['logg']        = 'logg'\n",
    "        unified_keywords['e_logg']      = 'e_logg'\n",
    "        unified_keywords['fe_h']        = '__Fe_H_'\n",
    "        unified_keywords['e_fe_h']      = 'e_fe_h'\n",
    "        unified_keywords['vmic']        = 'Vmic'\n",
    "        unified_keywords['e_vmic']      = 'e_vmic'\n",
    "        \n",
    "        # Errors are not reported on a star-by-star basis.\n",
    "        # We need to define the uncertainties globally.\n",
    "        # See Section 3.2 of Forsberg et al. (2019)\n",
    "        raw_data['e_teff'] = 50*np.ones(len(oid))\n",
    "        raw_data['e_logg'] = 0.15 * np.ones(len(oid))\n",
    "        raw_data['e_fe_h'] = 0.05 * np.ones(len(oid))\n",
    "        raw_data['e_vmic'] = 0.10 * np.ones(len(oid))\n",
    "\n",
    "        for element in ['O','Mg','Ca','Ti']:\n",
    "\n",
    "            # Solar abundanes from Asplund et al. (2009): http://adsabs.harvard.edu/abs/2009ARA%26A..47..481A\n",
    "            # as for Fig. 4 from Jönsson et al. (2017)\n",
    "            if element == 'O' : A_X = 8.69\n",
    "            if element == 'Mg': A_X = 7.60\n",
    "            if element == 'Ca': A_X = 6.34\n",
    "            if element == 'Ti': A_X = 4.95\n",
    "            raw_data[element.lower()+'_h'] = raw_data['A_'+element+'_'] - A_X\n",
    "            unified_keywords[element.lower()+'_h'] = element.lower()+'_h'\n",
    "            \n",
    "            # Errors are not reported on a star-by-star basis.\n",
    "            # We need to define the uncertainties globally.\n",
    "            # See Table 3 of Jönsson et al. (2017)\n",
    "            unified_keywords['e_'+element.lower()+'_h'] = 'e_'+element.lower()+'_h'\n",
    "            raw_data['e_'+element.lower()+'_h'] = np.ones(len(oid),dtype=float)\n",
    "            if element == 'O' : e_x_fe = np.sqrt(0.010**2 + 0.065**2 + 0.015**2 + 0.005**2)\n",
    "            if element == 'Mg': e_x_fe = np.sqrt(0.005**2 + 0.025**2 + 0.000**2 + 0.010**2)\n",
    "            if element == 'Ca': e_x_fe = 0.10 # not reported, therefore assumed at the end of Section 3.4\n",
    "            if element == 'Ti': e_x_fe = np.sqrt(0.070**2 + 0.010**2 + 0.000**2 + 0.020**2)\n",
    "            raw_data['e_'+element.lower()+'_h'] = e_x_fe\n",
    "\n",
    "            # Set a flag for each abundance and raise by 'flag_no_detection' if not finite\n",
    "            # also ensure that uncertainties are not finite for these stars\n",
    "            unified_keywords['flag_'+element.lower()+'_h'] = 'flag_'+element.lower()+'_h'\n",
    "            raw_data['flag_'+element.lower()+'_h'] = np.zeros(len(oid),dtype=int)\n",
    "            no_abundance_detected = np.isnan(raw_data[unified_keywords[element.lower()+'_h']])\n",
    "            raw_data['flag_'+element.lower()+'_h'][no_abundance_detected] += flag_no_detection\n",
    "            raw_data[unified_keywords['e_'+element.lower()+'_h']][no_abundance_detected] = np.nan\n",
    "\n",
    "        unified_keywords['Name']        = 'Name'       \n",
    "        \n",
    "    elif literature_file == 'Montelius_2022A&A...665A.135M':\n",
    "        \n",
    "        unified_keywords['author']      = 'author'\n",
    "        unified_keywords['ads_bibcode'] = 'ads_bibcode'\n",
    "        unified_keywords['tmass_id']    = 'tmass_id'\n",
    "        unified_keywords['ra']          = '_RA'\n",
    "        unified_keywords['dec']         = '_DE'\n",
    "\n",
    "        unified_keywords['teff']        = 'Teff'\n",
    "        unified_keywords['e_teff']      = 'e_teff'\n",
    "        unified_keywords['logg']        = 'logg'\n",
    "        unified_keywords['e_logg']      = 'e_logg'\n",
    "        unified_keywords['fe_h']        = '__Fe_H_'\n",
    "        unified_keywords['e_fe_h']      = 'e_fe_h'\n",
    "        unified_keywords['vmic']        = 'vmic'\n",
    "        unified_keywords['e_vmic']      = 'e_vmic'\n",
    "        \n",
    "        # Errors are not reported on a star-by-star basis.\n",
    "        # We need to define the uncertainties globally.\n",
    "        # See Section 3.1 of Montelius et al. (2022)\n",
    "        raw_data['e_teff'] = 50*np.ones(len(oid))\n",
    "        raw_data['e_logg'] = 0.10 * np.ones(len(oid))\n",
    "        raw_data['e_fe_h'] = 0.05 * np.ones(len(oid))\n",
    "        raw_data['e_vmic'] = 0.10 * np.ones(len(oid))\n",
    "\n",
    "        for element in ['Yb']:\n",
    "\n",
    "            # # Solar abundanes from Grevesse et al. (2007): http://adsabs.harvard.edu/abs/2015A%26A...573A..27G\n",
    "            # # as for Table 3 from Montelius et al. (2022)\n",
    "            # if element == 'Yb': A_X = 1.08\n",
    "            # raw_data[element.lower()+'_h'] = raw_data['A_'+element+'_'] - A_X\n",
    "            # unified_keywords[element.lower()+'_h'] = element.lower()+'_h'\n",
    "            unified_keywords[element.lower()+'_h'] = element.lower()+'_h'\n",
    "            raw_data[element.lower()+'_h'] = raw_data['__'+element+'_Fe_'] + raw_data['__Fe_H_']\n",
    "    \n",
    "            # Errors are not reported on a star-by-star basis.\n",
    "            # We need to define the uncertainties globally.\n",
    "            # See Section 3.6.2 of Montelius et al. (2022)\n",
    "            unified_keywords['e_'+element.lower()+'_h'] = 'e_'+element.lower()+'_h'\n",
    "            raw_data['e_'+element.lower()+'_h'] = np.ones(len(oid),dtype=float)\n",
    "            if element == 'Yb': e_x_fe = 0.11\n",
    "            if element == 'Ce': e_x_fe = 0.16 # same as Forsberg et al. (2019)\n",
    "            if element == 'Eu': e_x_fe = 0.15 # same as Forsberg et al. (2019)\n",
    "            raw_data['e_'+element.lower()+'_h'] = e_x_fe\n",
    "\n",
    "            # Set a flag for each abundance and raise by 'flag_no_detection' if not finite\n",
    "            # also ensure that uncertainties are not finite for these stars\n",
    "            unified_keywords['flag_'+element.lower()+'_h'] = 'flag_'+element.lower()+'_h'\n",
    "            raw_data['flag_'+element.lower()+'_h'] = np.zeros(len(oid),dtype=int)\n",
    "            no_abundance_detected = np.isnan(raw_data[unified_keywords[element.lower()+'_h']])\n",
    "            raw_data['flag_'+element.lower()+'_h'][no_abundance_detected] += flag_no_detection\n",
    "            raw_data[unified_keywords['e_'+element.lower()+'_h']][no_abundance_detected] = np.nan\n",
    "\n",
    "        unified_keywords['Star']        = 'Star'       \n",
    "        \n",
    "    elif literature_file == 'Nissen_2010A&A...511L..10N':\n",
    "        \n",
    "        unified_keywords['author']      = 'author'\n",
    "        unified_keywords['ads_bibcode'] = 'ads_bibcode'\n",
    "        unified_keywords['tmass_id']    = 'tmass_id'\n",
    "        unified_keywords['ra']          = 'RAJ2000'\n",
    "        unified_keywords['dec']         = 'DEJ2000'\n",
    "\n",
    "        unified_keywords['teff']        = 'Teff'\n",
    "        unified_keywords['e_teff']      = 'e_teff'\n",
    "        unified_keywords['logg']        = 'logg'\n",
    "        unified_keywords['e_logg']      = 'e_logg'\n",
    "        unified_keywords['fe_h']        = '__Fe_H_'\n",
    "        unified_keywords['e_fe_h']      = 'e_fe_h'\n",
    "        unified_keywords['vmic']        = 'ksi'\n",
    "        unified_keywords['e_vmic']      = 'e_vmic'\n",
    "        \n",
    "        # Errors are not reported on a star-by-star basis.\n",
    "        # We need to define the uncertainties globally.\n",
    "        # See Section 3 of Nissen & Schuster (2010)\n",
    "        raw_data['e_teff'] = 30 * np.ones(len(oid))\n",
    "        raw_data['e_logg'] = 0.05 * np.ones(len(oid))\n",
    "        raw_data['e_fe_h'] = 0.03 * np.ones(len(oid))\n",
    "        raw_data['e_vmic'] = 0.05 * np.ones(len(oid)) # Nissen 2024\n",
    "\n",
    "        for element in ['Na','Mg','Si','Ca','Ti','Cr','Ni']:\n",
    "\n",
    "            raw_data[element.lower()+'_h'] = raw_data['__'+element+'_Fe_'] + raw_data['__Fe_H_']\n",
    "            unified_keywords[element.lower()+'_h'] = element.lower()+'_h'\n",
    "            \n",
    "            # Errors are not reported on a star-by-star basis.\n",
    "            # We need to define the uncertainties globally.\n",
    "            # See Section 3 of Nissen & Schuster (2010)\n",
    "            unified_keywords['e_'+element.lower()+'_h'] = 'e_'+element.lower()+'_h'\n",
    "            if element in ['Na','Mg','Si']: e_x_fe = 0.035\n",
    "            if element in ['Ca','Ti','Cr']: e_x_fe = 0.02\n",
    "            if element in ['Ni']: e_x_fe = 0.01\n",
    "            raw_data['e_'+element.lower()+'_h'] = e_x_fe * np.ones(len(oid),dtype=float)\n",
    "\n",
    "            # Set a flag for each abundance and raise by 'flag_no_detection' if not finite\n",
    "            # also ensure that uncertainties are not finite for these stars\n",
    "            unified_keywords['flag_'+element.lower()+'_h'] = 'flag_'+element.lower()+'_h'\n",
    "            raw_data['flag_'+element.lower()+'_h'] = np.zeros(len(oid),dtype=int)\n",
    "            no_abundance_detected = np.isnan(raw_data[unified_keywords[element.lower()+'_h']])\n",
    "            raw_data['flag_'+element.lower()+'_h'][no_abundance_detected] += flag_no_detection\n",
    "            raw_data[unified_keywords['e_'+element.lower()+'_h']][no_abundance_detected] = np.nan\n",
    "\n",
    "        unified_keywords['Name']        = 'Name'\n",
    "        unified_keywords['Class']       = 'Class'\n",
    "        \n",
    "        \n",
    "    elif literature_file == 'Nissen_2011A&A...530A..15N':\n",
    "        \n",
    "        unified_keywords['author']      = 'author'\n",
    "        unified_keywords['ads_bibcode'] = 'ads_bibcode'\n",
    "        unified_keywords['tmass_id']    = 'tmass_id'\n",
    "        unified_keywords['ra']          = '_RA'\n",
    "        unified_keywords['dec']         = '_DE'\n",
    "\n",
    "        unified_keywords['teff']        = 'Teff'\n",
    "        unified_keywords['e_teff']      = 'e_teff'\n",
    "        unified_keywords['logg']        = 'logg'\n",
    "        unified_keywords['e_logg']      = 'e_logg'\n",
    "        unified_keywords['fe_h']        = '__Fe_H_'\n",
    "        unified_keywords['e_fe_h']      = 'e_fe_h'\n",
    "        unified_keywords['vmic']        = 'ksi'\n",
    "        unified_keywords['e_vmic']      = 'e_vmic'\n",
    "        \n",
    "        # Errors are not reported on a star-by-star basis.\n",
    "        # We need to define the uncertainties globally.\n",
    "        # See Section 3 of Nissen & Schuster (2010)\n",
    "        raw_data['e_teff'] = 30 * np.ones(len(oid))\n",
    "        raw_data['e_logg'] = 0.05 * np.ones(len(oid))\n",
    "        raw_data['e_fe_h'] = 0.03 * np.ones(len(oid))\n",
    "        raw_data['e_vmic'] = 0.05 * np.ones(len(oid)) # Nissen 2024\n",
    "\n",
    "        for element in ['Mn','Cu','Zn','Y','Ba']:\n",
    "\n",
    "            raw_data[element.lower()+'_h'] = raw_data['__'+element+'_Fe_'] + raw_data['__Fe_H_']\n",
    "            unified_keywords[element.lower()+'_h'] = element.lower()+'_h'\n",
    "            \n",
    "            # Errors are not reported on a star-by-star basis.\n",
    "            # We need to define the uncertainties globally.\n",
    "            # See Section 2.7 of Nissen & Schuster (2011)\n",
    "            unified_keywords['e_'+element.lower()+'_h'] = 'e_'+element.lower()+'_h'\n",
    "            if element == 'Mn': e_x_fe = 0.025\n",
    "            if element == 'Cu': e_x_fe = 0.035\n",
    "            if element == 'Zn': e_x_fe = 0.035\n",
    "            if element == 'Y' : e_x_fe = 0.035\n",
    "            if element == 'Ba': e_x_fe = 0.035\n",
    "            raw_data['e_'+element.lower()+'_h'] = e_x_fe * np.ones(len(oid),dtype=float)\n",
    "\n",
    "            # Set a flag for each abundance and raise by 'flag_no_detection' if not finite\n",
    "            # also ensure that uncertainties are not finite for these stars\n",
    "            unified_keywords['flag_'+element.lower()+'_h'] = 'flag_'+element.lower()+'_h'\n",
    "            raw_data['flag_'+element.lower()+'_h'] = np.zeros(len(oid),dtype=int)\n",
    "            no_abundance_detected = np.isnan(raw_data[unified_keywords[element.lower()+'_h']])\n",
    "            raw_data['flag_'+element.lower()+'_h'][no_abundance_detected] += flag_no_detection\n",
    "            raw_data[unified_keywords['e_'+element.lower()+'_h']][no_abundance_detected] = np.nan\n",
    "\n",
    "        unified_keywords['Name']        = 'Name'\n",
    "        unified_keywords['Class']       = 'Class'\n",
    "\n",
    "    elif literature_file == 'Nissen_2012A%26A...543A..28N':\n",
    "        \n",
    "        unified_keywords['author']      = 'author'\n",
    "        unified_keywords['ads_bibcode'] = 'ads_bibcode'\n",
    "        unified_keywords['tmass_id']    = 'tmass_id'\n",
    "        unified_keywords['ra']          = 'RAJ2000'\n",
    "        unified_keywords['dec']         = 'DEJ2000'\n",
    "\n",
    "        unified_keywords['teff']        = 'Teff'\n",
    "        unified_keywords['e_teff']      = 'e_teff'\n",
    "        unified_keywords['logg']        = 'logg'\n",
    "        unified_keywords['e_logg']      = 'e_logg'\n",
    "        unified_keywords['fe_h']        = '__Fe_H_'\n",
    "        unified_keywords['e_fe_h']      = 'e_fe_h'\n",
    "        unified_keywords['vmic']        = 'ksi'\n",
    "        unified_keywords['e_vmic']      = 'e_vmic'\n",
    "        \n",
    "        # Errors are not reported on a star-by-star basis.\n",
    "        # We need to define the uncertainties globally.\n",
    "        # See Section 3 of Nissen & Schuster (2010)\n",
    "        raw_data['e_teff'] = 30 * np.ones(len(oid))\n",
    "        raw_data['e_logg'] = 0.05 * np.ones(len(oid))\n",
    "        raw_data['e_fe_h'] = 0.03 * np.ones(len(oid))\n",
    "        raw_data['e_vmic'] = 0.05 * np.ones(len(oid)) # Nissen 2024\n",
    "\n",
    "        unified_keywords['a_li']        = 'A_Li'\n",
    "\n",
    "        # Errors are not reported on a star-by-star basis.\n",
    "        # We need to define the uncertainties globally.\n",
    "        # See end of Section 2.1 of Nissen & Schuster (2012)\n",
    "        raw_data['e_a_li'] = 0.04 * np.ones(len(oid))\n",
    "        unified_keywords['e_a_li']      = 'e_a_li'\n",
    "        \n",
    "        # Set a flag for each abundance and raise by 'flag_no_detection' if not finite\n",
    "        # also ensure that uncertainties are not finite for these stars\n",
    "        unified_keywords['flag_a_li'] = 'flag_a_li'\n",
    "        raw_data['flag_a_li'] = raw_data['flag_A_Li']\n",
    "        no_abundance_detected = np.isnan(raw_data[unified_keywords['a_li']])\n",
    "        raw_data['flag_a_li'][no_abundance_detected] += flag_no_detection\n",
    "        raw_data['e_a_li'][no_abundance_detected] = np.nan\n",
    "\n",
    "        unified_keywords['Name']        = 'Name'\n",
    "        unified_keywords['Class']       = 'Class'\n",
    "        \n",
    "    elif literature_file == 'Nissen_2014A&A...568A..25N':\n",
    "        \n",
    "        unified_keywords['author']      = 'author'\n",
    "        unified_keywords['ads_bibcode'] = 'ads_bibcode'\n",
    "        unified_keywords['tmass_id']    = 'tmass_id'\n",
    "        unified_keywords['ra']          = '_RA'\n",
    "        unified_keywords['dec']         = '_DE'\n",
    "\n",
    "        unified_keywords['teff']        = 'Teff'\n",
    "        unified_keywords['e_teff']      = 'e_teff'\n",
    "        unified_keywords['logg']        = 'logg'\n",
    "        unified_keywords['e_logg']      = 'e_logg'\n",
    "        unified_keywords['fe_h']        = '__Fe_H_'\n",
    "        unified_keywords['e_fe_h']      = 'e_fe_h'\n",
    "        unified_keywords['vmic']        = 'vt'\n",
    "        unified_keywords['e_vmic']      = 'e_vmic'\n",
    "        \n",
    "        # Errors are not reported on a star-by-star basis.\n",
    "        # We need to define the uncertainties globally.\n",
    "        # See larger values of Tables 6&7 of Nissen et al. (2014)\n",
    "        raw_data['e_teff'] = 35 * np.ones(len(oid))   \n",
    "        raw_data['e_logg'] = 0.06 * np.ones(len(oid))\n",
    "        raw_data['e_fe_h'] = 0.03 * np.ones(len(oid))\n",
    "        raw_data['e_vmic'] = 0.06 * np.ones(len(oid))\n",
    "\n",
    "        for element in ['C','O']:\n",
    "\n",
    "            if element == 'C':\n",
    "                unified_keywords[element.lower()+'_h'] = '__C_H_n'\n",
    "            if element == 'O':\n",
    "                unified_keywords[element.lower()+'_h'] = '__O_H_3n'\n",
    "\n",
    "            raw_data[element.lower()+'_h'] = raw_data[unified_keywords[element.lower()+'_h']]\n",
    "            \n",
    "            # Errors are not reported on a star-by-star basis.\n",
    "            # We need to define the uncertainties globally.\n",
    "            # See larger values of Tables 6&7 of Nissen et al. (2014)\n",
    "            unified_keywords['e_'+element.lower()+'_h'] = 'e_'+element.lower()+'_h'\n",
    "            if element == 'C': e_x_fe = 0.05\n",
    "            if element == 'O': e_x_fe = 0.044\n",
    "            raw_data['e_'+element.lower()+'_h'] = e_x_fe * np.ones(len(oid),dtype=float)\n",
    "\n",
    "            # Set a flag for each abundance and raise by 'flag_no_detection' if not finite\n",
    "            # also ensure that uncertainties are not finite for these stars\n",
    "            unified_keywords['flag_'+element.lower()+'_h'] = 'flag_'+element.lower()+'_h'\n",
    "            raw_data['flag_'+element.lower()+'_h'] = np.zeros(len(oid),dtype=int)\n",
    "            no_abundance_detected = np.isnan(raw_data[unified_keywords[element.lower()+'_h']])\n",
    "            raw_data['flag_'+element.lower()+'_h'][no_abundance_detected] += flag_no_detection\n",
    "            raw_data[unified_keywords['e_'+element.lower()+'_h']][no_abundance_detected] = np.nan\n",
    "\n",
    "        unified_keywords['HD']          = 'HD'\n",
    "        \n",
    "    elif literature_file == 'Nissen_2020A&A...640A..81N':\n",
    "        \n",
    "        unified_keywords['author']      = 'author'\n",
    "        unified_keywords['ads_bibcode'] = 'ads_bibcode'\n",
    "        unified_keywords['tmass_id']    = 'tmass_id'\n",
    "        unified_keywords['ra']          = '_RA'\n",
    "        unified_keywords['dec']         = '_DE'\n",
    "\n",
    "        unified_keywords['teff']        = 'Teff'\n",
    "        unified_keywords['e_teff']      = 'e_teff'\n",
    "        unified_keywords['logg']        = 'logg-sp'\n",
    "        unified_keywords['e_logg']      = 'e_logg'\n",
    "        unified_keywords['fe_h']        = '__Fe_H_'\n",
    "        unified_keywords['e_fe_h']      = 'e_fe_h'\n",
    "        unified_keywords['vmic']        = 'Vturb'\n",
    "        unified_keywords['e_vmic']      = 'e_vmic'\n",
    "        \n",
    "        # Errors are not reported on a star-by-star basis.\n",
    "        # We need to define the uncertainties globally.\n",
    "        # See Table 3 of Nissen et al. (2020)\n",
    "        subsets = [\n",
    "            (raw_data['__Fe_H_'] < -0.1),\n",
    "            (np.abs(raw_data['__Fe_H_']) <= 0.1),\n",
    "            (raw_data['__Fe_H_'] > 0.1),\n",
    "        ]\n",
    "        errors = dict()\n",
    "        errors['teff'] = [9,6,10]\n",
    "        errors['logg'] = [0.018,0.012,0.020]\n",
    "        errors['fe_h'] = [0.009,0.006,0.010]\n",
    "        errors['vmic'] = [0.03,0.02,0.03]\n",
    "        for key in errors.keys():\n",
    "            raw_data['e_'+key] = np.ones(len(oid))\n",
    "            for index, subset in enumerate(subsets):\n",
    "                raw_data['e_'+key][subset] = errors[key][index]\n",
    "                \n",
    "        errors['C']  = [0.016,0.013,0.024]\n",
    "        errors['O']  = [0.030,0.022,0.030]\n",
    "        errors['Na'] = [0.007,0.007,0.007]\n",
    "        errors['Mg'] = [0.014,0.011,0.014]\n",
    "        errors['Al'] = [0.008,0.008,0.010]\n",
    "        errors['Si'] = [0.007,0.007,0.008]\n",
    "        errors['Ca'] = [0.006,0.006,0.006]\n",
    "        errors['Ti'] = [0.007,0.007,0.009]\n",
    "        errors['Cr'] = [0.006,0.006,0.007]\n",
    "        errors['Ni'] = [0.006,0.006,0.006]\n",
    "        errors['Sr'] = [0.016,0.012,0.027]\n",
    "        errors['Y']  = [0.013,0.010,0.013]\n",
    "\n",
    "        for element in ['C','O','Na','Mg','Al','Si','Ca','Ti','Cr','Ni','Sr','Y']:\n",
    "\n",
    "            unified_keywords[element.lower()+'_h'] = element.lower()+'_h'\n",
    "            raw_data[element.lower()+'_h'] = raw_data['__'+element+'_Fe_'] + raw_data['__Fe_H_']\n",
    "            \n",
    "            unified_keywords['e_'+element.lower()+'_h'] = 'e_'+element.lower()+'_h'\n",
    "            raw_data['e_'+element.lower()+'_h'] = np.ones(len(oid))\n",
    "            for index, subset in enumerate(subsets):\n",
    "                raw_data['e_'+element.lower()+'_h'][subset] = errors[element][index]\n",
    "\n",
    "            # Set a flag for each abundance and raise by 'flag_no_detection' if not finite\n",
    "            # also ensure that uncertainties are not finite for these stars\n",
    "            unified_keywords['flag_'+element.lower()+'_h'] = 'flag_'+element.lower()+'_h'\n",
    "            raw_data['flag_'+element.lower()+'_h'] = np.zeros(len(oid),dtype=int)\n",
    "            no_abundance_detected = np.isnan(raw_data[unified_keywords[element.lower()+'_h']])\n",
    "            raw_data['flag_'+element.lower()+'_h'][no_abundance_detected] += flag_no_detection\n",
    "            raw_data[unified_keywords['e_'+element.lower()+'_h']][no_abundance_detected] = np.nan\n",
    "\n",
    "        unified_keywords['HD']          = 'HD'\n",
    "        \n",
    "    elif literature_file == 'Nissen_2024A&A...682A.116N':\n",
    "        \n",
    "        unified_keywords['author']      = 'author'\n",
    "        unified_keywords['ads_bibcode'] = 'ads_bibcode'\n",
    "        unified_keywords['tmass_id']    = 'tmass_id'\n",
    "        unified_keywords['ra']          = '_RA'\n",
    "        unified_keywords['dec']         = '_DE'\n",
    "\n",
    "        unified_keywords['teff']        = 'Teff'\n",
    "        unified_keywords['e_teff']      = 'e_teff'\n",
    "        unified_keywords['logg']        = 'logg'\n",
    "        unified_keywords['e_logg']      = 'e_logg'\n",
    "        unified_keywords['fe_h']        = '__Fe_H_3D'\n",
    "        unified_keywords['e_fe_h']      = 'e_fe_h'\n",
    "        unified_keywords['vmic']        = 'Vturb'\n",
    "        unified_keywords['e_vmic']      = 'e_vmic'\n",
    "        \n",
    "        # Errors are not reported on a star-by-star basis.\n",
    "        # We need to define the uncertainties globally.\n",
    "        # See Section 3 of Nissen & Schuster (2010)\n",
    "        raw_data['e_teff'] = 30 * np.ones(len(oid))\n",
    "        raw_data['e_logg'] = 0.05 * np.ones(len(oid))\n",
    "        raw_data['e_fe_h'] = 0.03 * np.ones(len(oid))\n",
    "        raw_data['e_vmic'] = 0.05 * np.ones(len(oid))\n",
    "\n",
    "        for element in ['Sc','V','Co']:\n",
    "\n",
    "            raw_data[element.lower()+'_h'] = raw_data['__'+element+'_H_']\n",
    "            unified_keywords[element.lower()+'_h'] = element.lower()+'_h'\n",
    "            \n",
    "            # Errors are not reported on a star-by-star basis.\n",
    "            # We need to define the uncertainties globally.\n",
    "            # See Section 2 of Nissen et al. (2024)\n",
    "            unified_keywords['e_'+element.lower()+'_h'] = 'e_'+element.lower()+'_h'\n",
    "            if element == 'Sc': e_x_fe = 0.03\n",
    "            if element == 'V' : e_x_fe = 0.03\n",
    "            if element == 'Co': e_x_fe = 0.04\n",
    "            raw_data['e_'+element.lower()+'_h'] = e_x_fe * np.ones(len(oid),dtype=float)\n",
    "\n",
    "            # Set a flag for each abundance and raise by 'flag_no_detection' if not finite\n",
    "            # also ensure that uncertainties are not finite for these stars\n",
    "            unified_keywords['flag_'+element.lower()+'_h'] = 'flag_'+element.lower()+'_h'\n",
    "            raw_data['flag_'+element.lower()+'_h'] = np.zeros(len(oid),dtype=int)\n",
    "            no_abundance_detected = np.isnan(raw_data[unified_keywords[element.lower()+'_h']])\n",
    "            raw_data['flag_'+element.lower()+'_h'][no_abundance_detected] += flag_no_detection\n",
    "            raw_data[unified_keywords['e_'+element.lower()+'_h']][no_abundance_detected] = np.nan\n",
    "\n",
    "        unified_keywords['Star']        = 'Star'\n",
    "        unified_keywords['Pop']         = 'Pop'\n",
    "        \n",
    "    else:\n",
    "        print('You still need to extend this function for '+literature_file)\n",
    "        print(raw_data.keys())\n",
    "        return raw_data\n",
    "    \n",
    "    for unified_keyword in unified_keywords.keys():\n",
    "        if unified_keyword in ['author','ads_bibcode','tmass_id','Name','Class','Star']:\n",
    "            unified_table[unified_keyword] = np.array(raw_data[unified_keywords[unified_keyword]], dtype=str)\n",
    "        elif (unified_keyword in ['HIP','HD']) | (unified_keyword[:5] == 'flag_') | (unified_keyword in ['teff','e_teff']):\n",
    "            unified_table[unified_keyword] = np.array(raw_data[unified_keywords[unified_keyword]], dtype=np.int32)\n",
    "        else:\n",
    "            try:\n",
    "                unified_table[unified_keyword] = np.array(raw_data[unified_keywords[unified_keyword]], dtype=np.float32)\n",
    "            except:\n",
    "                print(unified_keyword, raw_data[unified_keywords[unified_keyword]].dtype)\n",
    "                print('Could not convert '+unified_keyword+' to np.float32. Assuming it is an integer instead')\n",
    "\n",
    "    created = []\n",
    "    # Create lite tables for individual elements\n",
    "    for element in elements:\n",
    "        if (element.lower()+'_h' in unified_table.keys()) | ('a_'+element.lower() in unified_table.keys()):\n",
    "            \n",
    "            # Create 'literature_abundances/'+element+'/'\n",
    "            Path('literature_abundances/'+element+'/').mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            unified_table_lite = Table()\n",
    "            for key in unified_table.keys():\n",
    "                # if A(Li)\n",
    "                if key in ['a_'+element.lower(),'e_a_'+element.lower(),'flag_a_'+element.lower()]:\n",
    "                    unified_table_lite[key] = unified_table[key]\n",
    "                # if [X/H], check if X = Fe or *this* element\n",
    "                elif key[-2:] == '_h':\n",
    "                    if key in ['fe_h','e_fe_h']:\n",
    "                        unified_table_lite[key] = unified_table[key]\n",
    "                    elif key in [element.lower()+'_h','e_'+element.lower()+'_h','flag_'+element.lower()+'_h']:\n",
    "                        unified_table_lite[key] = unified_table[key]\n",
    "                    else:\n",
    "                        # elements that are not *this* element\n",
    "                        pass\n",
    "                else:\n",
    "                    unified_table_lite[key] = unified_table[key]\n",
    "\n",
    "            unified_table_lite.write('literature_abundances/'+element+'/'+literature_file+'_'+element+'.fits',overwrite=True)\n",
    "            created.append(element)\n",
    "    print('For '+literature_file+' created: '+\", \".join(created))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "f6965f6f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Battistini_2015A&A...577A...9B created: Sc, V, Mn, Fe, Co\n",
      "For Battistini_2016A&A...586A..49B created: Fe, Sr, Zr, La, Ce, Nd, Sm, Eu\n",
      "For Bensby_2014A&A...562A..71B created: O, Na, Mg, Al, Si, Ca, Ti, Cr, Fe, Ni, Zn, Y, Ba\n",
      "For Bensby_2017A&A...605A..89B created: Na, Mg, Al, Si, Ca, Ti, Cr, Fe, Ni, Zn, Y, Ba\n",
      "For Bensby_2018A%26A...615A.151B created: Li, Fe\n",
      "For Bensby_2020A&A...634A.130B created: Li, Fe\n",
      "For Bensby_2021A&A...655A.117B created: C, O, Fe\n",
      "For Forsberg_2019A&A...631A.113F created: Fe, Zr, La, Ce, Eu\n",
      "For Forsberg_2022A&A...666A.125F created: Fe, Mo\n",
      "For Joensson_2017A&A...598A.100J created: O, Mg, Ca, Ti, Fe\n",
      "For Montelius_2022A&A...665A.135M created: Fe, Yb\n",
      "For Nissen_2010A&A...511L..10N created: Na, Mg, Si, Ca, Ti, Cr, Fe, Ni\n",
      "For Nissen_2011A&A...530A..15N created: Mn, Fe, Cu, Zn, Y, Ba\n",
      "For Nissen_2012A%26A...543A..28N created: Li, Fe\n",
      "For Nissen_2014A&A...568A..25N created: C, O, Fe\n",
      "For Nissen_2020A&A...640A..81N created: C, O, Na, Mg, Al, Si, Ca, Ti, Cr, Fe, Ni, Sr, Y\n",
      "For Nissen_2024A&A...682A.116N created: Sc, V, Fe, Co\n"
     ]
    }
   ],
   "source": [
    "for literature_file in available_literature:\n",
    "    table = unify_keywords(literature_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba73d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437b2c7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
